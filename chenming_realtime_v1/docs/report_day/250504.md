项目讲义

1.[Test01_FlinkCDC.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdim%2FTest01_FlinkCDC.java)

前端埋点数据,拉取数据 分别从flume 采集 到kfk 的日志数据 发送到下游 flink 
业务数据是cdc 从 mysql 的binlog 去拿数据
拿到数据 之后 设置 checkpoint 将数据源打包成数据流

useSSL=false	禁用 SSL 连接	提高性能，测试环境可用
decimal.handling.mode=double	将 decimal 转为 double	避免 JSON 序列化失败
time.precision.mode=connect	使用 connect 模式时间戳	兼容 Kafka Connect 时间格式
chunk.key-column=id	    分片主键	控制快照分片粒度，提高并发

CheckPoint + StateBackend：保障作业失败重启时的状态恢复。
Kafka DeliveryGuarantee.AT_LEAST_ONCE：至少一次投递语义，防止数据丢失。
CDC Source 自动重连机制：MySQL 断开连接后自动恢复，不中断作业。



问题与解决方案（常见答辩提问）
Q1: 如何保证数据一致性？
使用 Checkpoint + Kafka Sink 的 AT_LEAST_ONCE 语义，结合幂等消费者可以做到最终一致。
Q2: 如果 MySQL 表结构发生变化怎么办？
CDC 会自动感知 DDL 变化，并在事件中携带 schema 信息，下游需支持 schema evolution（如使用 Avro）。
Q3: 如何处理大字段或者二进制字段？
示例中的 price 字段是 base64 编码的 binary 类型，建议提前确认字段类型，必要时进行解码处理。
Q4: 并行度设置多少合适？为什么？
一般根据数据量大小和 Kafka 分区数决定。本例设为 4，适配中小规模数据集。



2.[DimApp.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdim%2FDimApp.java)

在实时数仓建设中，我们通常需要将维度数据进行缓存或持久化存储，以便后续的 DWD、DWS 层做维度建模和关联查询。
本项目基于 Flink 构建了一个 维度数据同步系统（Dim App），其核心目标是：
实时捕获业务数据库的变更；
根据配置表动态决定哪些表作为维度表；
将维度数据写入 HBase，供后续使用；
支持动态建表、删表、修改配置，实现灵活管理。

env.setParallelism(1);
env.enableCheckpointing(5000L, CheckpointingMode.EXACTLY_ONCE);
env.setStateBackend(new HashMapStateBackend());
env.getCheckpointConfig().setCheckpointStorage("hdfs://cdh01:8020/...");
设置并行度为 1：确保单线程下对 HBase 的操作安全。
开启 Checkpoint：保障 Exactly Once 语义，防止数据丢失。
使用 HashMapStateBackend + HDFS Checkpoint 存储：轻量且适合小状态作业。

BroadcastStream<TableProcessDim> broadcastDS = tpDs.broadcast(mapStateDescriptor);
将配置流广播出去，使每条业务数据都能获取到当前所有配置信息。

字段裁剪	只保留 sink_columns 指定字段，减少网络传输与 HBase IO
广播机制	配置信息全量广播，保证每个 operator 能快速匹配维度规则
RichMapFunction 管理 HBase 连接	避免频繁创建连接，提高资源利用率
CheckPoint 状态后端配置	保障作业失败恢复时状态一致性
精确控制 HBase 建表时机	只有当配置存在时才建表，避免无效资源占用

Q1: 为什么设置并行度为 1？
A：因为 HBase 写入操作具有幂等性和并发限制，为了简化开发逻辑和避免并发冲突，设置为单并行度。后续可考虑使用分区策略提升并发。

Q2: 如何处理 HBase 建表失败或重复建表？
A：我们在建表前先检查是否存在，若存在则删除再重建；同时记录日志，便于运维排查。未来可引入幂等机制或版本控制。

Q3: 如果配置表中有大量配置怎么办？会影响性能吗？
A：目前采用广播方式，会把所有配置发送给每个 task，如果配置过多会导致内存压力。建议分批次加载，或引入外部存储如 Redis 缓存配置。

Q4: 为什么使用 HashMapStateBackend？
A：该 StateBackend 更适合小规模状态管理，简单易维护。对于大数据量场景，应使用 RocksDBStateBackend 并开启增量快照。


3.[DwdBaseDb.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdBaseDb.java)
本项目基于 Flink 构建了一个 DWD Base DB 处理模块，其核心目标是：

实时消费 Kafka 中的原始数据库变更日志；
根据配置表中的规则动态决定每条数据应该写入哪个 DWD 主题；
对数据字段进行裁剪，保留有效字段；
将结果输出到下游系统（如 Kafka、HBase、ClickHouse 等）；

public static void main(String[] args) throws Exception {
new DwdBaseDb().start(10019, "dwd_base_db", Constant.TOPIC_DB);
}
使用封装好的 BaseApp 基类启动程序。

processElement(String s, ProcessFunction.Context ctx, Collector<JSONObject> out)
解析 JSON 字符串，提取操作类型 op。
过滤掉 bootstrap 类型的初始化数据。
输出格式统一为 JSONObject，便于后续处理。

使用 Flink CDC 读取 MySQL 配置表 realtime_v1_config.table_process_dwd。
配置表字段包括：
source_table: 原始表名（如 favor_info）
source_type: 操作类型（insert / update / delete）
sink_table: 目标 DWD 表名（如 dwd_interaction_favor_add）
sink_columns: 要写入的字段列表

优化点	说明
字段裁剪	只保留 sink_columns 指定字段，减少网络传输与下游处理压力
广播机制	配置信息全量广播，保证每个 operator 能快速匹配规则
双流关联策略	使用 BroadcastConnectedStream 实现高效动态路由
配置热加载	配置变更无需重启任务，提升系统灵活性
数据过滤	排除 bootstrap 初始化数据，减少无效计算

为什么要排除 bootstrap 类型的数据？
A：因为 bootstrap 是 Kafka Connect 的初始快照数据，已经通过批处理方式处理过了，在实时流中我们只关注增量变更，这样可以减少重复处理和资源浪费。

Q2: 如何实现动态配置更新？
A：我们使用了 Flink 的 Broadcast State 模式，将配置流作为广播流，主业务流在处理时可以访问最新的配置信息。当配置发生变化时，会自动更新广播状态，从而实现热更新。

Q3: 如果配置表中没有对应规则怎么办？
A：如果没有匹配的配置规则，这条数据就不会被收集，也不会发送到下游系统。相当于做了“白名单”控制，只有配置了的表才会被处理。

Q4: 为什么使用 BroadcastProcessFunction 而不是 CoProcessFunction？
A：因为配置流通常变化频率较低，而主业务流非常高频。使用 Broadcast 可以将配置分发给每个 operator，并且支持高效的 read-only 访问，适合这种一对多的动态路由场景。


4.[DwdInteractionCommentInfo.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdInteractionCommentInfo.java)

本模块聚焦于“用户评论”行为的实时采集与处理，构建了一个名为 dwd_interaction_comment_info 的加购评论事实表。
通过关联字典表，补全评价维度信息，为后续分析提供支持。

Kafka(chenming_db) → Flink SQL Table(db)
↓
过滤出 comment_info 表的数据 → comment_info 视图
↓
HBase(dim_base_dic) ← 维度表
↓
左连接 → 获取 appraise_name（评分名称）
↓
写入 Kafka(dwd_interaction_comment_info_chenming)

Upsert Kafka Sink	支持主键去重更新的写入方式


✅ 1. 环境初始化

StreamTableEnvironment tenv = StreamTableEnvironment.create(env);
初始化 Flink 流执行环境；
设置并行度为 1（方便调试，生产建议根据数据量调整）；
创建 StreamTableEnvironment 用于执行 Flink SQL；
✅ 2. 定义 Kafka Source 表
CREATE TABLE db (
after Map<String,String>,
source  Map<String,String>,
op  String,
ts_ms  bigint,
before MAP<string,string>,
proc_time  AS proctime()
) WITH ( ... )
定义 Kafka Source 表结构；
使用 JSON 格式解析消息体；
提取操作类型 op、时间戳 ts_ms、源表名 source['table']；
添加 proc_time 字段用于后续窗口计算或事件时间处理；

✅ 3. 过滤评论数据
select
after['id'] as id,
after['user_id'] as user_id,
after['sku_id'] as sku_id,
after['appraise'] as appraise,
after['comment_txt'] as comment_txt,
ts_ms as ts,
proc_time
from db where source['table'] = 'comment_info'
从 Kafka 数据中筛选出 comment_info 表的数据；
提取评论 ID、用户 ID、商品 ID、评分、评论内容等关键字段；
注册为临时视图 comment_info，便于后续查询使用；
✅ 4. 定义 HBase Dim Table
CREATE TABLE hbase (
dic_code String,
info ROW<dic_name String>,
PRIMARY KEY (dic_code) NOT ENFORCED
) WITH ( ... )
定义 HBase 表结构；
主键为 dic_code，表示字典码；
info.dic_name 是字典名称；
用于补充评论中的 appraise 字段对应的人类可读名称；
✅ 5. 关联评论数据与字典表
SELECT  
id, user_id, sku_id, appraise, dic.dic_name as appraise_name, comment_txt, ts
FROM comment_info AS c
left join hbase as dic
ON c.appraise = dic.dic_code
使用 LEFT JOIN 将评论数据与字典表关联；
补全 appraise_name 字段，提升数据语义表达能力；
准备好写入下游系统的格式；
✅ 6. 定义 Kafka Sink 表
CREATE TABLE dwd_interaction_comment_info_chenming (
id string,
user_id string,
sku_id string,
appraise string,
appraise_name string,
comment_txt string,
ts bigint,
PRIMARY KEY (id) NOT ENFORCED
) WITH ( 'connector' = 'kafka', 'format' = 'json', 'kafka.transactional-id-prefix' = '...' )
定义目标 Kafka 表结构；
设置主键 id，使用 Upsert Kafka Sink 实现幂等写入；
支持按主键去重更新，避免重复插入；
✅ 7. 写入目标表
table3.executeInsert(Constant.TOPIC_DWD_INTERACTION_COMMENT_INFO);
执行插入操作，将处理后的数据写入 Kafka 目标主题；
后续可以被 DWS 层或其他消费者订阅处理；

优化
字段裁剪	只保留业务需要的字段，减少网络传输与存储开销
LEFT JOIN 补维	提升数据可读性，增强下游分析效率
Upsert Sink	支持主键去重更新，保证数据一致性
Proc Time 时间列	方便做实时监控与延迟统计
SQL 抽象封装	提高代码可维护性和复用性

五、常见答辩问题与回答思路
Q1: 为什么要使用 Flink SQL 而不是 DataStream API？
A：Flink SQL 更适合做结构化数据处理，语法简洁、易维护、开发效率高，特别适合 ETL 场景。对于复杂逻辑也可以混合使用 UDF 或自定义函数扩展。

Q2: 为什么使用 Upsert Kafka Sink？
A：因为评论数据可能存在多次更新（如修改评论内容），使用 Upsert 模式可以基于主键实现幂等写入，避免重复插入或丢失更新。

Q3: 如果字典表不存在怎么办？会不会导致数据丢失？
A：不会。我们使用的是 LEFT JOIN，即使字典表中没有对应的 appraise 码值，也能保留原始数据，只是 appraise_name 字段为空，不影响核心数据输出。

Q4: 如何保障数据准确性？
A \Kafka Source 使用 Exactly-Once 语义；
Upsert Sink 支持幂等写入；
整个流程端到端保证数据一致性；
日志打印和监控告警机制辅助排查异常；


📈 六、未来优化方向
引入 Schema Registry
统一管理数据格式，增强兼容性。
增加 Checkpoint 配置
提高容错能力，保障任务重启不丢数据。
接入 Flink Web UI 监控
实时查看数据吞吐、延迟、状态大小等指标。
支持多维表关联
除了字典表，还可以关联用户表、商品表等，丰富数据维度。

5.[DwdTradeCartAdd.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdTradeCartAdd.java)

本模块聚焦于“用户加购”行为的实时采集与处理，构建了一个名为 dwd_trade_cart_add 的加购事实表。
通过识别加购动作（新增或数量增加），为后续交易行为分析提供支撑。


过滤加购数据（核心逻辑）

select
after['id'] id,
after['user_id'] user_id,
after['sku_id'] sku_id,
if(op='c', after['sku_num'], CAST((CAST(after['sku_num'] AS INT) - CAST(before['sku_num'] AS INT)) AS STRING )) sku_num,
ts_ms
from db
where source['table']='cart_info'  
and (
op = 'c'
or op = 'r'
or (op='u' and before['sku_num'] is not null and (CAST(after['sku_num'] AS INT) > CAST(before['sku_num'] AS INT)))
)
🔍 逻辑拆解：
只保留 cart_info 表的操作记录
过滤出以下三种情况作为“加购”行为：
op = 'c'：插入新记录（新增购物车项）
op = 'r'：快照读（首次同步历史数据时）
op = 'u'：更新操作，且商品数量增加（排除减少或无效更新）
计算加购数量：
如果是新增（op='c'），直接取当前 sku_num
如果是更新（数量增加），则用新旧差值作为本次加购数量

字段裁剪	只保留业务需要的字段，减少网络传输与存储开销
逻辑判断精准过滤	准确识别加购行为，避免无效数据进入下游系统
SKU 数量差值计算	精准反映每次加购的真实数量变化
Upsert Sink	支持主键去重更新，保证数据一致性
SQL 抽象封装	提高代码可维护性和复用性

Q1: 为什么要区分 op='c', op='r', op='u'？
A：

op='c' 是插入操作，代表用户第一次添加商品；
op='r' 是快照读，用于初始化同步；
op='u' 是更新操作，只有当商品数量增加时才视为“加购”，避免误判减购或无效更新。
Q2: SKU 数量为什么要做差值计算？
A：因为一次更新操作可能只是修改了购物车中的其他字段（如备注），但只有当商品数量增加时才是真正的“加购”。通过差值可以更准确地统计用户的加购行为。

Q3: 如何保障数据准确性？
Kafka Source 使用 Exactly-Once 语义；
Upsert Sink 支持幂等写入；
整个流程端到端保证数据一致性；
日志打印和监控告警机制辅助排查异常；

6.[DwdTradeOrderCancelDetail.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdTradeOrderCancelDetail.java)
本模块聚焦于“用户取消订单”行为的实时采集与处理，构建了一个名为 dwd_trade_order_cancel 的取消订单明细事实表。通过关联下单明细表，
补全订单商品维度信息，为后续分析用户提供行为路径、流失原因等提供数据支持。


✅ . 定义下单明细表 dwd_trade_order_detail
CREATE TABLE dwd_trade_order_detail (...) WITH ('connector' = 'kafka', ...)
该表是从 Kafka 中读取的下单明细表；
包含订单 ID、用户 ID、SKU ID、商品名称、省份 ID、优惠券信息、金额分摊等字段；
是下游分析的重要基础表；
✅ . 关联取消订单与下单明细
SELECT  
od.id,
od.order_id,
od.user_id,
od.sku_id,
od.sku_name,
od.province_id,
od.activity_id,
od.activity_rule_id,
od.coupon_id,
date_format(FROM_UNIXTIME(CAST(oc.operate_time AS BIGINT) / 1000), 'yyyy-MM-dd') AS order_cancel_date_id,
date_format(FROM_UNIXTIME(CAST(oc.operate_time AS BIGINT) / 1000), 'yyyy-MM-dd hh:mm:ss') AS operate_time,
od.sku_num,
od.split_original_amount,
od.split_activity_amount,
od.split_coupon_amount,
od.split_total_amount,
oc.ts
FROM dwd_trade_order_detail od
JOIN order_cancel oc
ON od.order_id = oc.id
使用 JOIN 将取消订单行为与下单明细进行关联；
补全商品、用户、活动、优惠券等维度信息；
转换时间戳格式，便于下游按日期统计分析；
构建完整的取消订单明细数据；

为什么要将取消订单与下单明细表进行 JOIN？
A：因为原始取消订单数据只包含订单级别信息，缺乏商品、用户、地区等详细维度。通过 JOIN 下单明细表，可以丰富数据内容，提升数据分析的粒度和准确性。

Q3: 如果订单没有对应的明细怎么办？会不会导致数据丢失？
A：不会。我们在 SQL 中使用的是 INNER JOIN，意味着只有存在匹配的订单明细才会输出。如果要保留所有取消订单记录，应改为 LEFT JOIN 并处理可能为空的字段。

7.[DwdTradeOrderDetail.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdTradeOrderDetail.java)
本模块聚焦于“用户下单”行为的采集与整合，构建了一个名为 dwd_trade_order_detail 的订单明细事实表。
通过将订单主表、订单明细、活动信息、优惠券信息进行关联，形成一个结构清晰、字段丰富的宽表，为后续交易行为分析、
用户画像、商品推荐等场景提供高质量的数据支撑。

1）定义 Kafka Source 表（db）
定义 Kafka Source 表结构；
使用 JSON 格式解析消息体；
提取操作类型 op、时间戳 ts_ms、源表名 source['table']；
添加 proc_time 字段用于后续窗口计算或事件时间处理；


2）过滤出订单明细数据（order_detail）
抽取订单明细表的关键字段：SKU ID、SKU 名称、数量、金额分摊等；
计算 split_original_amount（分摊原始金额）= 数量 × 单价；
注册为临时视图 order_detail，供后续 JOIN 使用；

3）. 过滤出订单信息（order_info）
获取订单级别的维度信息：用户 ID、省份 ID；
注册为临时视图 order_info；

4）过滤出订单活动信息（order_detail_activity）
获取订单参与的营销活动信息；
注册为临时视图 order_detail_activity；

5）过滤出订单优惠券信息（order_detail_coupon）
获取订单使用的优惠券信息；
注册为临时视图 order_detail_coupon

6） 多表关联构建宽表
使用 INNER JOIN 和 LEFT JOIN 联合四张表；
构建完整订单明细宽表；
补全用户、地区、活动、优惠券等维度；
时间戳格式化，方便按天统计；

Q1: 为什么要将四张表进行 JOIN？
A：为了构建一个完整的订单明细宽表，补全用户、地区、活动、优惠券等维度信息，提升数据分析的粒度和准确性，避免下游多次查询和关联，提高整体查询效率。

Q2: 为什么使用 LEFT JOIN？
A：因为并不是每个订单明细都一定有对应的活动或优惠券记录，使用 LEFT JOIN 可以保留所有订单明细，即使没有活动或优惠券也能输出基础信息。

Q3: 如果源数据中没有 after 字段怎么办？
A：在 CDC 数据中，after 表示操作后的最新状态，如果为空可能是删除操作（op='d'）。我们在过滤时已排除掉这类数据，只保留新增和更新的数据。


8.[DwdTradeOrderPaySucDetail.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdTradeOrderPaySucDetail.java)

✅ 1. 初始化环境

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);
StreamTableEnvironment tenv = StreamTableEnvironment.create(env);
初始化 Flink 流执行环境；
设置并行度为 1（便于调试）；
创建 StreamTableEnvironment 用于执行 Flink SQL；
✅ 2. 读取下单事实表（dwd_trade_order_detail）

CREATE TABLE dwd_trade_order_detail (
...
) WITH Kafka
消费上一个作业生成的 dwd_trade_order_detail_chenming 主题；
定义字段结构，包含订单 ID、SKU、用户 ID、地区、活动优惠券等；
添加 Watermark 和 Event Time 字段，用于时间语义处理；
✅ 3. 读取原始 DB 表（chenming_db）
CREATE TABLE db ( ... ) WITH Kafka
消费原始 CDC 数据；
提取 after 字段中的业务数据；
同样添加 et（event time）和 watermark 用于时间语义处理；
✅ 4. 过滤出支付成功数据（payment_info）
SELECT
after['user_id'] user_id,
after['order_id'] order_id,
after['payment_type'] payment_type,
after['callback_time'] callback_time,
proc_time,
ts_ms as ts,
et
FROM db WHERE source['table']='payment_info'
从原始 DB 数据中过滤出 payment_info 表的记录；
获取支付成功的核心字段：订单 ID、用户 ID、支付方式、回调时间等；
注册为临时视图 payment_info，供后续 JOIN 使用；
✅ 5. 读取 HBase 字典表（base_dic）
CREATE TABLE base_dic (
dic_code STRING,
info ROW<dic_name STRING>,
PRIMARY KEY (dic_code) NOT ENFORCED
) WITH HBase
从 HBase 中读取字典数据；
用于将支付类型的编码转换为中文名称；
使用 lookup join 实现实时补维；
✅ 6. 多表关联构建支付成功宽表
SELECT  
od.id order_detail_id,
od.order_id,
od.user_id,
od.sku_id,
od.sku_name,
od.province_id,
od.activity_id,
od.activity_rule_id,
od.coupon_id,
pi.payment_type payment_type_code,
dic.dic_name payment_type_name,
pi.callback_time,
od.sku_num,
od.split_original_amount,
od.split_activity_amount,
od.split_coupon_amount,
od.split_total_amount split_payment_amount,
pi.ts
FROM payment_info pi
JOIN dwd_trade_order_detail od ON pi.order_id = od.order_id
AND od.et >= pi.et - INTERVAL '30' MINUTE
AND od.et <= pi.et + INTERVAL '5' SECOND
JOIN base_dic FOR SYSTEM_TIME AS OF pi.proc_time AS dic ON pi.payment_type = dic.dic_code
Interval Join：限定订单创建时间和支付回调时间之间的合理范围（±30分钟），避免无效数据；
Lookup Join：使用 SYSTEM_TIME AS OF 实现 HBase 字典表的实时补维；
构建完整支付成功宽表，包含商品、用户、地区、支付方式等维度信息；
✅ 7. 将结果写入 Kafka Sink 表
sql
深色版本
CREATE TABLE dwd_trade_order_payment_success (...) WITH Upsert Kafka
定义目标 Kafka 表结构；
设置主键 order_detail_id，使用 Upsert Kafka Sink 实现幂等写入；
支持按主键去重更新，避免重复插入；
⚙️ 四、性能优化点（答辩加分项）
优化点	说明
Event Time + Watermark	防止乱序数据影响计算结果
Interval Join	控制订单与支付的时间窗口，提高数据准确性
Lookup Join	实时查询字典表，减少冗余字段存储
Upsert Sink	支持主键去重更新，保证数据一致性
JSON Ignore Parse Errors	容错处理异常 JSON 格式数据
💡 五、常见答辩问题与回答思路
Q1: 为什么要用 Interval Join？
A：为了确保订单和支付行为在合理的时间范围内发生，防止历史订单被错误匹配。例如：支付时间不能早于订单创建时间太早或太晚，否则可能是异常数据。

Q2: 为什么使用 Lookup Join 而不是提前把字典表加载进内存？
A：因为字典表可能会动态更新，使用 Lookup Join 可以实现实时同步，而不会丢失最新的字典信息。如果使用内存缓存则需要额外维护刷新机制，复杂度高。

Q3: 如果支付信息先于订单到达怎么办？
A：由于使用了 Interval Join 的时间约束条件，这种情况会被自动过滤掉。只有当订单时间在支付前后一定时间内才会被保留，保障数据合理性。

9.[DwdTradeOrderRefund.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdTradeOrderRefund.java)


本模块聚焦于“用户退单”行为的采集与整合，构建了一个名为 dwd_trade_order_refund 的退单事务事实表。

通过将退单明细、订单信息、字典表三者进行多维度关联，形成一个结构清晰、字段丰富的宽表，为后续 DWS/ADS 层提供完整的退单行为数据支持。

二、技术选型说明
组件	作用
Flink SQL	使用 Flink SQL 实现流式 ETL，简化开发逻辑
Kafka Source/Sink	原始数据源输入通道及结果输出通道
HBase Lookup Join	实时读取字典表信息，补全退单类型名称
Proctime 时间语义	当前使用处理时间（proc_time）进行 lookup join
Map 类型字段提取	从 CDC 数据中提取 after/before 字段构建业务模型
🔍 三、程序逻辑详解（答辩重点）
✅ 1. 初始化环境
java
深色版本
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);
StreamTableEnvironment tenv = StreamTableEnvironment.create(env);
初始化 Flink 流执行环境；
设置并行度为 1（便于调试）；
创建 StreamTableEnvironment 用于执行 Flink SQL；
✅ 2. 读取原始 DB 表（chenming_db）
sql
深色版本
CREATE TABLE db ( ... ) WITH Kafka
消费原始 CDC 数据；
提取 after 字段中的业务数据；
定义 proc_time 字段用于后续 lookup join；
✅ 3. 过滤出退单数据（order_refund_info）
sql
深色版本
SELECT  
after['id'] id,
after['user_id'] user_id,
after['order_id'] order_id,
after['sku_id'] sku_id,
after['refund_type'] refund_type,
after['refund_num'] refund_num,
after['refund_amount'] refund_amount,
after['refund_reason_type'] refund_reason_type,
after['refund_reason_txt'] refund_reason_txt,
after['create_time'] create_time,
proc_time,
ts_ms as ts
FROM db
WHERE source['table']='order_refund_info'
从原始 DB 数据中过滤出 order_refund_info 表的记录；
获取退单的核心字段：订单 ID、SKU、退单类型、数量金额、原因等；
注册为临时视图 order_refund_info，供后续 JOIN 使用；
✅ 4. 过滤订单表中的退单数据（order_info）
sql
深色版本
SELECT  
after['id'] id,
after['province_id'] province_id
FROM db
WHERE source['table']='order_info'
从原始 DB 数据中过滤出 order_info 表的记录；
提取省份 ID，用于补充地理位置维度；
注册为临时视图 order_info，供后续 JOIN 使用；
✅ 5. 读取 HBase 字典表（base_dic）
sql
深色版本
CREATE TABLE base_dic (
dic_code STRING,
info ROW<dic_name STRING>,
PRIMARY KEY (dic_code) NOT ENFORCED
) WITH HBase
从 HBase 中读取字典数据；
用于将退单类型编码和退单原因类型编码转换为中文名称；
使用 lookup join 实现实时补维；
✅ 6. 多表关联构建退单宽表
sql
深色版本
SELECT  
ri.id,
ri.user_id,
ri.order_id,
ri.sku_id,
oi.province_id,
DATE_FORMAT(FROM_unixtime(CAST(ri.create_time AS BIGINT) / 1000), 'yyyy-MM-dd') date_id,
ri.create_time,
ri.refund_type,
dic1.dic_name refund_type_name,
ri.refund_reason_type,
dic2.dic_name refund_reason_type_name,
ri.refund_reason_txt,
ri.refund_num,
ri.refund_amount,
ri.ts
FROM order_refund_info ri
JOIN order_info oi ON ri.order_id = oi.id
JOIN base_dic FOR SYSTEM_TIME AS OF ri.proc_time AS dic1 ON ri.refund_type = dic1.dic_code
JOIN base_dic FOR SYSTEM_TIME AS OF ri.proc_time AS dic2 ON ri.refund_reason_type = dic2.dic_code
普通 JOIN：退单信息与订单信息按订单 ID 关联；
Lookup JOIN × 2：分别获取退单类型和退单原因类型的中文名称；
构建完整退单宽表，包含商品、地区、退单类型、金额、原因等维度信息；
✅ 7. 将结果写入 Kafka Sink 表（注释部分）
sql
深色版本
CREATE TABLE dwd_trade_order_refund (...) WITH Upsert Kafka
定义目标 Kafka 表结构；
设置主键 id，使用 Upsert Kafka Sink 实现幂等写入；
支持按主键去重更新，避免重复插入；
⚙️ 四、性能优化点（答辩加分项）
优化点	说明
Proctime + Lookup Join	实现字典表的实时同步，减少冗余字段存储
Upsert Sink	支持主键去重更新，保证数据一致性
字段提取优化	只提取所需字段，减少内存占用
合理使用 Map 提取方式	避免解析整个 JSON，提高效率
日期格式化处理	在 Flink 内部完成时间维度拆分，提升下游查询效率
💡 五、常见答辩问题与回答思路
Q1: 为什么使用 Lookup Join 而不是提前把字典表加载进内存？
A：因为字典表可能会动态更新，使用 Lookup Join 可以实现实时同步，而不会丢失最新的字典信息。如果使用内存缓存则需要额外维护刷新机制，复杂度高。

Q2: 如果退单信息先于订单到达怎么办？
A：由于这里是普通的 INNER JOIN，如果订单信息未到达，会导致退单记录被过滤掉。可以通过引入 LEFT JOIN 并设置状态 TTL 来实现延迟关联。

Q3: 为什么没有使用 Event Time 和 Watermark？
A：当前使用的是 proc_time，即处理时间，适用于对时效性要求较高、但容忍一定延迟的场景。如果对事件时间有强依赖，可以考虑加入 ts_ms 字段作为 event time，并添加 watermark。


10.
[DwdTradeRefundPaySucDetail.java](..%2F..%2Fstream-realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fcm%2Fdwd%2FDwdTradeRefundPaySucDetail.java)

“用户退款成功”这一关键业务事件，构建了一个名为 dwd_trade_refund_payment_success 的退款成功事务事实表。

通过将退款支付记录、退单信息、订单信息三者进行多维度关联，形成一个结构清晰、字段丰富的宽表，为后续 DWS/ADS 层提供完整的退款行为数据支持。

技术选型说明
组件	作用
Flink SQL	使用 Flink SQL 实现流式 ETL，简化开发逻辑
Kafka Source/Sink	原始数据源输入通道及结果输出通道
HBase Lookup Join	实时读取字典表信息，补全支付类型名称
Proctime + State TTL	控制状态生命周期，避免内存溢出
Upsert Sink	支持主键去重更新，保证数据一致性
🔍 三、程序逻辑详解（答辩重点）
✅ 1. 初始化环境 & 状态配置

tEnv.getConfig().setIdleStateRetention(Duration.ofSeconds(5));
设置状态存活时间（TTL）为 5 秒；
避免因长时间保留无用状态导致内存占用过高；
适用于短窗口或低延迟场景；
✅ 2. 读取原始 DB 表（topic_db）

CREATE TABLE chenming_db ( ... ) WITH Kafka
消费原始 CDC 数据；
包含多个业务表变更日志；
提取 after, before, source, op 字段用于判断状态变化；
✅ 3. 过滤退款支付成功数据（refund_payment）

SELECT  
after['id'] id,
after['order_id'] order_id,
after['sku_id'] sku_id,
after['payment_type'] payment_type,
after['callback_time'] callback_time,
after['total_amount'] total_amount,
pt, ts_ms
FROM chenming_db
WHERE source['table']='refund_payment'
AND op='u'
AND before['refund_status'] IS NOT NULL
AND after['refund_status']='1602'
过滤出 refund_payment 表中状态变为 “1602（退款成功）” 的记录；
获取退款成功的订单 ID、SKU、支付方式、回调时间、金额等核心字段；
注册为临时视图 refund_payment，供后续 JOIN 使用；
✅ 4. 过滤退单成功数据（order_refund_info）

SELECT  
after['order_id'] order_id,
after['sku_id'] sku_id,
after['refund_num'] refund_num
FROM chenming_db
WHERE source['table']='order_refund_info'
AND op='u'
AND before['refund_status'] IS NOT NULL
AND after['refund_status']='0705'
过滤出 order_refund_info 表中状态变为 “0705（已退款）” 的记录；
获取退单数量等补充字段；
注册为临时视图 order_refund_info；
✅ 5. 过滤订单退款成功数据（order_info）

SELECT  
after['id'] id,
after['user_id'] user_id,
after['province_id'] province_id
FROM chenming_db
WHERE source['table']='order_info'
AND op='u'
AND before['order_status'] IS NOT NULL
AND after['order_status']='1006'
过滤出 order_info 表中状态变为 “1006（订单关闭并退款完成）” 的记录；
获取用户 ID 和省份 ID 等维度信息；
注册为临时视图 order_info；
✅ 6. 多表关联构建退款成功宽表

SELECT  
rp.id,
oi.user_id,
rp.order_id,
rp.sku_id,
oi.province_id,
rp.payment_type,
dic.info.dic_name payment_type_name,
DATE_FORMAT(TO_TIMESTAMP_LTZ(CAST(rp.callback_time AS BIGINT), 3), 'yyyy-MM-dd') date_id,
rp.callback_time,
ori.refund_num,
rp.total_amount,
rp.ts_ms
FROM refund_payment rp
JOIN order_refund_info ori ON rp.order_id = ori.order_id AND rp.sku_id = ori.sku_id
JOIN order_info oi ON rp.order_id = oi.id
JOIN base_dic FOR SYSTEM_TIME AS OF rp.pt AS dic ON rp.payment_type = dic.dic_code
普通 JOIN × 3：实现退款支付、退单、订单信息三表关联；
Lookup JOIN：获取支付类型的中文名称；
构建完整退款成功宽表，包含商品、用户、地区、支付方式、金额、退单数量等维度信息；
✅ 7. 将结果写入 Kafka Sink 表
   
CREATE TABLE dwd_trade_refund_payment_success (...) WITH Upsert Kafka
定义目标 Kafka 表结构；
设置主键 id，使用 Upsert Kafka Sink 实现幂等写入；
支持按主键去重更新，避免重复插入；
⚙️ 四、性能优化点（答辩加分项）
优化点	说明
State TTL 设置	控制状态生命周期，防止内存泄漏
Upsert Sink	支持主键去重更新，保证数据一致性
字段提取优化	只提取所需字段，减少内存占用
合理使用 Map 提取方式	避免解析整个 JSON，提高效率
日期格式化处理	在 Flink 内部完成时间维度拆分，提升下游查询效率
💡 五、常见答辩问题与回答思路
Q1: 为什么选择使用 pt 时间而不是 ts_ms 来做 lookup join？
A：pt 是 Flink 自动生成的处理时间字段，适合用于 lookup join 场景。而 ts_ms 是事件时间，如果存在乱序或延迟，可能导致无法正确匹配字典表数据。

Q2: 如果退款支付先于退单到达怎么办？
A：由于这里是普通的 INNER JOIN，如果退单信息未到达，会导致退款记录被过滤掉。可以通过引入 LEFT JOIN 并设置状态 TTL 来实现延迟关联。

Q3: 如何保障数据一致性？
A：

Kafka Source 使用 Exactly-Once 语义；
Upsert Sink 支持幂等写入；
整个流程端到端保证数据一致性；
日志打印和监控告警机制辅助排查异常；
Q4: 为什么要使用 HBase 字典表？为什么不直接在 Flink 中缓存？
A：字典表可能会动态更新，使用 HBase Lookup Join 可以实现实时同步，而不会丢失最新的字典信息。如果使用内存缓存则需要额外维护刷新机制，复杂度高。